\thusetup{
  %******************************
  % 注意：
  %   1. 配置里面不要出现空行
  %   2. 不需要的配置信息可以删除
  %******************************
  %
  %=====
  % 秘级
  %=====
  secretlevel={秘密},
  secretyear={10},
  %
  %=========
  % 中文信息
  %=========
  ctitle={大规模知识图谱表示学习},
  cdegree={工学学士},
  cdepartment={计算机科学与技术系},
  cmajor={计算机科学与技术专业},
  cauthor={韩旭},
  csupervisor={刘知远助理教授},
  % cassosupervisor={陈文光教授}, 
  % 副指导老师
  % ccosupervisor={某某某教授}, 
  % 联合指导老师
  % 日期自动使用当前时间，若需指定按如下方式修改：
  % cdate={超新星纪元},
  %
  % 博士后专有部分
  cfirstdiscipline={计算机科学与技术},
  cseconddiscipline={系统结构},
  postdoctordate={2013年7月——2017年7月},
  id={编号}, % 可以留空： id={},
  udc={UDC}, % 可以留空
  catalognumber={分类号}, % 可以留空
  %
  %=========
  % 英文信息
  %=========
  etitle={Large-scale Knowledge Graph Representation Learning},
  % 这块比较复杂，需要分情况讨论：
  % 1. 学术型硕士
  %    edegree：必须为Master of Arts或Master of Science（注意大小写）
  %             “哲学、文学、历史学、法学、教育学、艺术学门类，公共管理学科
  %              填写Master of Arts，其它填写Master of Science”
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 2. 专业型硕士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：“工程硕士填写工程领域，其它专业学位不填写此项”
  % 3. 学术型博士
  %    edegree：Doctor of Philosophy（注意大小写）
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 4. 专业型博士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：不填写此项
  edegree={Bachelor of Engineering},
  emajor={Computer Science and Technology},
  eauthor={Han Xu},
  esupervisor={Assistant Professor Liu Zhiyuan},
  % eassosupervisor={Chen Wenguang},
  % 日期自动生成，若需指定按如下方式修改：
  % edate={December, 2005}
  %
  % 关键词用“英文逗号”分割
  ckeywords={大规模,知识图谱,表示学习,联合学习,并行加速},
  ekeywords={large-scale, knowledge Graph, representation learning, joint learning, parallel acceleration}
}

% 定义中英文摘要和关键字
\begin{cabstract}

  近年来，在人工智能及数据挖掘的部分领域中，为了抽象现实世界的知识并以一种统一载体进行结构化存储，学术界和工业界提出了知识图谱，并在此基础上进行了广泛研究。知识图谱中蕴含的丰富结构化信息对许多任务有很好的辅助效果，如问答系统、网络搜索，逻辑推演等。在广泛发挥作用的同时，知识图谱也存在部分问题亟需解决：第一，图谱虽然总量巨大但仍有更大一批知识存在缺失，需要进行深入的填补；第二，如何将图谱的结构化信息融入到当下松散的特征模型中去，需要有效的融合算法；第三，知识图谱巨大的规模对算法时间复杂度要求较高，需要高效的模型能够在有限的时间内对图谱进行操作。而解决这些问题的核心是能够高效、准确地将图谱表示成计算机能够理解的数字抽象，并进行快速特征融合。本文针对大规模知识图谱表示学习以及特征融合分别提出了训练框架。图谱表示学习框架通过底层优化以及图谱划分，将以往的图谱模型转化成多线程训练模型，未来可以衍生为分布式的训练方法。与此同时，我们提出加权点采样算法和位移负例采样算法来取代原有的边采样及负例采样，从而能够对图中不同的实体和事实赋予不同的注意，缓解幂率分布带来的长尾影响，以及尽可能的合并运算以便进行加速。特征融合框架采用了平行结合的方式与文本神经网络模型结合，对比于传统的串行结合方式，能够更快的引入文本信息来丰富图谱内容并进行信息融合。相关的实验表明，本文提出的框架能够在效果不降低的情况下极快的加速图谱学习，并和文本模型有很好的融合效果使得融合之后两者都有显著效果提升。我们已经开源了部分代码\footnote{https://github.com/thunlp/Fast-TransX \\ https://github.com/thunlp/TensorFlow-TransX}以便于其他领域研究者使用。

\end{cabstract}

% 如果习惯关键字跟在摘要文字后面，可以用直接命令来设置，如下：
% \ckeywords{\TeX, \LaTeX, CJK, 模板, 论文}

\begin{eabstract}

   In recent years, in the field of artificial intelligence and data mining, people organize structural knowledge about the world under the unified framework and construct various large-scale knowledge graphs for the knowledge storage. Due to structural information, knowledge graphs are playing an important role in many applications such as question answering, web search, logical inference, etc. However, there are also some problems need to be solved: (1) Most large-scale knowledge graphs are usually far from completion and need to be further extended. (2) We need effective methods to fuse knowledge and text features so that we can incorporate knowledge graphs into practical applications. (3) We need efficient models to operate on the large-scale graphs within the limited time. The key to solving these problems is to learn large-scale knowledge graph representations. In this paper, we propose frameworks to embed large-scale knowledge graphs and incorporate them into text models. For the knowledge representation framework, we divide the overall knowledge graph into several parts and adopt models for multi-threaded training. Besides, we propose the offset-based negative sampling to replace original negative sampling algorithms. The new sampling mechanism can alleviate the long tail effect and merge arithmetic operations for acceleration. For the feature fusion framework, we use a parallel training method to combine neural netwroks and knowledge graph models. The experimental results show that our frameworks can accelerate the existing knowledge modes dramatically without reducing the accuracy. At the same time, our methods can effectively perform joint representation learning and obtain more informative knowledge and text representations, which significantly outperforms other baseline methods. Some resource codes have been released \footnote{https://github.com/thunlp/Fast-TransX \\ https://github.com/thunlp/TensorFlow-TransX} so that researchers can easily adopt our framework for their own works.

\end{eabstract}

% \ekeywords{\TeX, \LaTeX, CJK, template, thesis}
